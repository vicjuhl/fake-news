{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate distribtion between fake and reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.mappings import labels\n",
    "labels = {\n",
    "    \"fake\": \"fake\",\n",
    "    \"bias\": \"fake\",\n",
    "    \"junksci\": \"fake\",\n",
    "    \"hate\": \"fake\",\n",
    "    \"reliable\": \"reliable\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dataframe with id, orig_type, type, and duplicates=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_file = pl.Path(os.path.abspath('')).parent.parent.resolve() / \"data_files/corpus/news_cleaned_2018_02_13.csv\"\n",
    "dups_path = pl.Path(os.path.abspath('')).parent.parent.resolve() / \"data_files/corpus/duplicates.csv\"\n",
    "to_path = pl.Path(os.path.abspath('')).parent.parent.resolve() / \"data_files/corpus/\"\n",
    "file_name = \"duplicate_distribution.csv\"\n",
    "\n",
    "\n",
    "def get_dups_distribution(from_file: pl.Path, dups_path: pl.Path, to_path: pl.Path, file_name: pl.Path) -> None:\n",
    "    \"\"\"Get id, type and orig_type for all duplicates in source dataset.\n",
    "\n",
    "    Writes a file to the directory\n",
    "    \"\"\"\n",
    "    # load id and type from source file\n",
    "    df = pd.read_csv(from_file, usecols=['id','type'])\n",
    "    print(f\"Nulls found: \\n{df.isnull().sum()}\")\n",
    "    df = df[df['type'].notnull()]               # filter\n",
    "    df = df.rename(columns={'type':'orig_type'})\n",
    "    df['type'] = df['orig_type'].map(labels)\n",
    "\n",
    "    label_counts = df['orig_type'].value_counts()\n",
    "\n",
    "    # load id from duplicates file\n",
    "    df_dups = pd.read_csv(dups_path)\n",
    "    df_dups['duplicate'] = True                 # add duplicate column\n",
    "    # merge source file with duplicate check\n",
    "    df = df.merge(df_dups,\"left\",on=\"id\")       # match duplicate IDs in source file\n",
    "    df = df[df['duplicate'].notnull()]          # filter results to show only duplicate IDs and orig_type\n",
    "    df.to_csv(to_path / file_name, index=False)\n",
    "    print(f\"\\n Distribution of duplicate IDs over type was written to {to_path}/{file_name}\")\n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/hj39jtf940s2chfk3bnbmsqm0000gn/T/ipykernel_14392/1657095944.py:12: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(from_file, usecols=['id','type'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls found: \n",
      "id      1654589\n",
      "type    2057872\n",
      "dtype: int64\n",
      "\n",
      " Distribution of duplicate IDs over type was written to /Users/victor/Documents/Uddannelse/Repos/fake-news/data_files/corpus/duplicate_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "orig_label_count = get_dups_distribution(from_file, dups_path, to_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias        322657\n",
       "fake        128710\n",
       "reliable    103089\n",
       "junksci      31784\n",
       "hate          9741\n",
       "Name: orig_type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(to_path / file_name)\n",
    "df['orig_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bias', 'fake', 'reliable', 'junksci', 'hate'],\n",
       " [1138998, 894746, 1913222, 117467, 76496],\n",
       " [322657, 128710, 103089, 31784, 9741]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_labs = [\"bias\",\"fake\",\"reliable\",\"junksci\",\"hate\"]\n",
    "\n",
    "pre_label_count = []\n",
    "for lab in orig_labs:\n",
    "    pre_label_count.append(orig_label_count[lab])\n",
    "\n",
    "dups_label_count = []\n",
    "for i in range(len(orig_labs)):\n",
    "    dups_label_count.append(df['orig_type'].value_counts()[i])\n",
    "\n",
    "data_export = []\n",
    "data_export.append(orig_labs)\n",
    "data_export.append(pre_label_count)\n",
    "data_export.append(dups_label_count)\n",
    "data_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< Updated upstream:src/notebooks/dups_distribution.ipynb
   "source": []
=======
   "source": [
    "# imported data\n",
    "data = [['bias', 'fake', 'reliable', 'junksci', 'hate'],\n",
    " [1_138_998, 894_746, 1_913_222, 117_467, 76_496],\n",
    " [322_657, 128_710, 103_089, 31_784, 9_741]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups = pd.DataFrame()\n",
    "df_dups['orig_type'] = data[0]\n",
    "df_dups['source_articles'] = data[1]\n",
    "df_dups['duplicates'] = data[2]\n",
    "df_dups['included_articles'] = df_dups['source_articles']-df_dups['duplicates']\n",
    "df_dups['percentage_removed'] = df_dups['duplicates']/df_dups['source_articles']\n",
    "df_dups = df_dups.sort_values(by='source_articles', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups.duplicates.sum() / df_dups.source_articles.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "X_axis = np.arange(len(df_dups['orig_type']))\n",
    "\n",
    "plt.bar(X_axis, df_dups['duplicates'], 0.5, bottom=df_dups['included_articles'], color=\"dodgerblue\", label='Duplicates removed', alpha=0.5)\n",
    "plt.bar(X_axis, df_dups['included_articles'], 0.5, label = 'Articles included')\n",
    "\n",
    "\n",
    "plt.xticks(X_axis, df_dups['orig_type'])\n",
    "plt.xlabel(\"Original Type\")\n",
    "plt.ylabel(\"No. of Articles\")\n",
    "plt.title(\"FakeNewsCorpus\")\n",
    "plt.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (5,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content length by Label type in Fake News Corpus\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Find the order\n",
    "\n",
    "\n",
    "# create a box plot of 'content_len' by 'orig_type'\n",
    "ax = plt.figure().add_axes([0,0,0.8,0.5])\n",
    "\n",
    "ax.bar(df_dups['orig_type'], df_dups['percentage_removed'])\n",
    "\n",
    "\n",
    "# add labels and title to the plot\n",
    "plt.xlabel('Label Type')\n",
    "plt.ylabel('Content Length')\n",
    "#plt.title('Distribution of Content Length by Label Type') # titles/captions are added in \n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop reliable label\n",
    "df_dups_ex_rel = df_dups.drop(2)\n",
    "\n",
    "# check percentage removed of fake group\n",
    "df_dups_ex_rel['duplicates'].sum()/df_dups_ex_rel['source_articles'].sum()"
   ]
>>>>>>> Stashed changes:src/dups_distribution.ipynb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
